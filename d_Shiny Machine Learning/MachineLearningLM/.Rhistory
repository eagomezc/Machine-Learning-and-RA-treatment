View(lm_profiles)
View(lm_profiles_scale)
lm_profiles_scale$responses <- factor(lm_profile[-1, ]$groups)
View(lm_profiles_scale)
#--------------------------------------------- ML LIPIDOMIC DATA ---------------------------------------------------#
# This script compiles different machine learning methodologies to be run using lipid mediator profiling data. This
# script only includes the training step without the evaluation step.
# It runs SVM, random forest, LASSO analysis and basic general linear models.
#---> LIBRARY LOAD:
if (!require('randomForest')) install.packages('randomForest'); library('randomForest')
if (!require('ggplot2')) install.packages('ggplot2'); library('ggplot2')
if (!require('snowfall')) install.packages('snowfall')
if (!require('neldermead')) install.packages('neldermead')
if (!require('optimbase')) install.packages('optimbase')
if (!require('classyfire')) install.packages("classyfire_0.1-2.tar.gz",
repos = NULL,
type = "source"); library('classyfire')
set.seed(415) # To get same results even with the random part.
#---> INPUT FILE:
lm_profile <- read.table(
file = "../input/2_randomForest_(RF_models)_data.tsv",
header = TRUE,
row.names = 1, # Specify that the first column is row names.
sep = "\t",
stringsAsFactors = FALSE)
# Separates the profiles data by lipid mediators types:
substrates <- unique(unname(unlist(lm_profile[1, -1])))
# Creates data frames for each subset:
dataframes_list <- list("lm_profile" = lm_profile)
for (i in 1:length(substrates)) {
substrate <- lm_profile[ ,lm_profile[1, ]== substrates[[i]]]
assign(substrates[[i]], substrate)
}
if (exists('DHA') == TRUE) {dataframes_list[["DHA"]] <- DHA}
if (exists('n3DPA') == TRUE) {dataframes_list[["n3DPA"]] <- n3DPA}
if (exists('RvE') == TRUE) {dataframes_list[["RvE"]] <- RvE}
if (exists('AA') == TRUE) {dataframes_list[["AA"]] <- AA}
for (j in 1:length(dataframes_list)) {
lm_profiles <- dataframes_list[[j]]
# Save all the values as numeric:
lm_profile_number <- sapply(lm_profiles[-1, -1], function(x) as.numeric(x))
row.names(lm_profile_number) <- row.names(lm_profiles[-1, -1])
# Scale the data because is better when you are running Machine Learning models:
lm_profiles_scale <- as.data.frame(scale(lm_profile_number, center = FALSE, scale = TRUE))
# If all the values from one column are the same, the scalation will give you NA. For those cases, to avoid errors,
# replace the NA for zeros.
lm_profiles_scale[is.na(lm_profiles_scale)] <- 0
# Add the classification variable to the data frame (Responder and non responder):
# Getting the explanatory (x) and response (y) variable. By explanatory, it means all the data that can explain why a
# patient response or not to the treatment (the lipid meadiator profiles) and the response variable is if the
# patients response or not to the treatment. In random Forest you have to create a formula where the response
# variable is explain in terms of the explanatory variable (responses ~ everything else).
lm_profiles_scale$responses <- factor(lm_profile[-1, ]$groups)
# Make sure that column names do not represent a problem to randomForest making them a valid name to R.
names(lm_profiles_scale) <- make.names(names(lm_profiles_scale))
#---> MACHINE LEARNING (randomForest R):
# In Random Forests the idea is to decorrelate the several trees which are generated on the different bootstrapped
# samples from training Data and then reduce the variance in the trees by averaging them.
# Averaging the trees also improve the perfomance of decision trees on Test Set and eventually avoid overfitting.
# The idea is to build lots of trees in such a way to make the correlation between the trees smaller.
# BEST MTRY:
# mtry is the number of variables available for splitting at each tree node. Random Forest creates several trees,
# each one using different variables to create the best version of it. With mtry we can define how many variables
# the data is split to create the different trees.
# More: https://stats.stackexchange.com/questions/102867/random-forest-mtry-question
# In this case we defined the lipid mediators to create a loop to define which mtry is the best one for our model.
oob_error <- double(ncol(lm_profiles_scale) - 1) #Define number of variable. -1 is because the last column is responses.
for (mtry in 1:(ncol(lm_profiles_scale) - 1)) {
# NOTE:
# importance = TRUE creates the plot of the important variables, that can gave us an idea, based on the
# decrease of the accuracy of the models, what lipid mediators are contributing to make a better model.
rf_lm_profiles_scales <- randomForest(responses ~ ., data = lm_profiles_scale, mtry = mtry,
importance = TRUE, ntree = 10000)
oob_error[mtry] <- 100 - ((rf_lm_profiles_scales$err.rate[10000])*100)
}
# Define the best mtry according to the best prediction value.
final_mtry <- which.max(oob_error)
# Run the model again with the right mtry value.
rf_lm_profiles_final <- randomForest(responses ~ ., data = lm_profiles_scale, mtry = final_mtry,
importance = TRUE, ntree = 10000)
# Get the confusion matrix of the model, sensitivity and specificity:
confusion_lm_profiles <- as.data.frame(rf_lm_profiles_final$confusion)
confusion_lm_profiles[is.na(confusion_lm_profiles)] <- 0
# Calculates sensitivity, specificity and AUC.
sensitivity_lm_profiles <- confusion_lm_profiles[2, 2]/(confusion_lm_profiles[2, 2] + confusion_lm_profiles[2, 1])
specificity_lm_profiles <- confusion_lm_profiles[1, 1]/(confusion_lm_profiles[1, 1] + confusion_lm_profiles[1, 2])
}
View(confusion_lm_profiles)
dataframes_list[[j]]
name(dataframes_list[[j]])
names(dataframes_list[[j]])
names(dataframes_list)
names(dataframes_list)[j]
assign(paste("ntree_", names(dataframes_list)[j], sep = ""), ggLinePlot)
ggLinePlot <- ggplot(data = tree_table, aes(x = Ensemble, y = AvgAcc)) +
geom_line(linetype = "solid", size = 1.2, color = "navyblue") +
scale_x_continuous(name = "Trees") +
scale_y_continuous(name = "Average Percent Accuracy", limits=c(50, 100)) +
theme(axis.title.y = element_text(size = 40, colour = "black"),
axis.title.x = element_text(size = 40, colour = "black"),
axis.text.x  =  element_text(size = 40, colour = "black"), # Put color to the labels
axis.text.y  = element_text(size = 40, colour = "black"), # Put color to the labels
axis.line = element_line(colour = 'black', size = 1.5), # Color and thickness of axis
axis.ticks = element_line(colour = "black", size = 1.5), # Color and thickness of every axis sep.
legend.position = ("none"),
panel.background = element_rect(fill = "white"),
axis.ticks.length = unit(0.4, "cm"))
tree_table <- data.frame(Ensemble = c(1:10000),
OBB = rf_lm_profiles_final$err.rate[, 1],
AvgAcc = 100 - ((rf_lm_profiles_final$err.rate[, 1])*100))
ggLinePlot <- ggplot(data = tree_table, aes(x = Ensemble, y = AvgAcc)) +
geom_line(linetype = "solid", size = 1.2, color = "navyblue") +
scale_x_continuous(name = "Trees") +
scale_y_continuous(name = "Average Percent Accuracy", limits=c(50, 100)) +
theme(axis.title.y = element_text(size = 40, colour = "black"),
axis.title.x = element_text(size = 40, colour = "black"),
axis.text.x  =  element_text(size = 40, colour = "black"), # Put color to the labels
axis.text.y  = element_text(size = 40, colour = "black"), # Put color to the labels
axis.line = element_line(colour = 'black', size = 1.5), # Color and thickness of axis
axis.ticks = element_line(colour = "black", size = 1.5), # Color and thickness of every axis sep.
legend.position = ("none"),
panel.background = element_rect(fill = "white"),
axis.ticks.length = unit(0.4, "cm"))
ggLinePlot
#--------------------------------------------- ML LIPIDOMIC DATA ---------------------------------------------------#
# This script compiles different machine learning methodologies to be run using lipid mediator profiling data. This
# script only includes the training step without the evaluation step.
# It runs SVM, random forest, LASSO analysis and basic general linear models.
#---> LIBRARY LOAD:
if (!require('randomForest')) install.packages('randomForest'); library('randomForest')
if (!require('ggplot2')) install.packages('ggplot2'); library('ggplot2')
if (!require('snowfall')) install.packages('snowfall')
if (!require('neldermead')) install.packages('neldermead')
if (!require('optimbase')) install.packages('optimbase')
if (!require('classyfire')) install.packages("classyfire_0.1-2.tar.gz",
repos = NULL,
type = "source"); library('classyfire')
set.seed(415) # To get same results even with the random part.
#---> INPUT FILE:
lm_profile <- read.table(
file = "../input/2_randomForest_(RF_models)_data.tsv",
header = TRUE,
row.names = 1, # Specify that the first column is row names.
sep = "\t",
stringsAsFactors = FALSE)
# Separates the profiles data by lipid mediators types:
substrates <- unique(unname(unlist(lm_profile[1, -1])))
# Creates data frames for each subset:
dataframes_list <- list("lm_profile" = lm_profile)
for (i in 1:length(substrates)) {
substrate <- lm_profile[ ,lm_profile[1, ]== substrates[[i]]]
assign(substrates[[i]], substrate)
}
if (exists('DHA') == TRUE) {dataframes_list[["DHA"]] <- DHA}
if (exists('n3DPA') == TRUE) {dataframes_list[["n3DPA"]] <- n3DPA}
if (exists('RvE') == TRUE) {dataframes_list[["RvE"]] <- RvE}
if (exists('AA') == TRUE) {dataframes_list[["AA"]] <- AA}
for (j in 1:length(dataframes_list)) {
lm_profiles <- dataframes_list[[j]]
# Save all the values as numeric:
lm_profile_number <- sapply(lm_profiles[-1, -1], function(x) as.numeric(x))
row.names(lm_profile_number) <- row.names(lm_profiles[-1, -1])
# Scale the data because is better when you are running Machine Learning models:
lm_profiles_scale <- as.data.frame(scale(lm_profile_number, center = FALSE, scale = TRUE))
# If all the values from one column are the same, the scalation will give you NA. For those cases, to avoid errors,
# replace the NA for zeros.
lm_profiles_scale[is.na(lm_profiles_scale)] <- 0
# Add the classification variable to the data frame (Responder and non responder):
# Getting the explanatory (x) and response (y) variable. By explanatory, it means all the data that can explain why a
# patient response or not to the treatment (the lipid meadiator profiles) and the response variable is if the
# patients response or not to the treatment. In random Forest you have to create a formula where the response
# variable is explain in terms of the explanatory variable (responses ~ everything else).
lm_profiles_scale$responses <- factor(lm_profile[-1, ]$groups)
# Make sure that column names do not represent a problem to randomForest making them a valid name to R.
names(lm_profiles_scale) <- make.names(names(lm_profiles_scale))
#---> MACHINE LEARNING (randomForest R):
# In Random Forests the idea is to decorrelate the several trees which are generated on the different bootstrapped
# samples from training Data and then reduce the variance in the trees by averaging them.
# Averaging the trees also improve the perfomance of decision trees on Test Set and eventually avoid overfitting.
# The idea is to build lots of trees in such a way to make the correlation between the trees smaller.
# BEST MTRY:
# mtry is the number of variables available for splitting at each tree node. Random Forest creates several trees,
# each one using different variables to create the best version of it. With mtry we can define how many variables
# the data is split to create the different trees.
# More: https://stats.stackexchange.com/questions/102867/random-forest-mtry-question
# In this case we defined the lipid mediators to create a loop to define which mtry is the best one for our model.
oob_error <- double(ncol(lm_profiles_scale) - 1) #Define number of variable. -1 is because the last column is responses.
for (mtry in 1:(ncol(lm_profiles_scale) - 1)) {
# NOTE:
# importance = TRUE creates the plot of the important variables, that can gave us an idea, based on the
# decrease of the accuracy of the models, what lipid mediators are contributing to make a better model.
rf_lm_profiles_scales <- randomForest(responses ~ ., data = lm_profiles_scale, mtry = mtry,
importance = TRUE, ntree = 10000)
oob_error[mtry] <- 100 - ((rf_lm_profiles_scales$err.rate[10000])*100)
}
# Define the best mtry according to the best prediction value.
final_mtry <- which.max(oob_error)
# Run the model again with the right mtry value.
rf_lm_profiles_final <- randomForest(responses ~ ., data = lm_profiles_scale, mtry = final_mtry,
importance = TRUE, ntree = 10000)
# Get the confusion matrix of the model, sensitivity and specificity:
confusion_lm_profiles <- as.data.frame(rf_lm_profiles_final$confusion)
confusion_lm_profiles[is.na(confusion_lm_profiles)] <- 0
# Calculates sensitivity, specificity and AUC.
sensitivity_lm_profiles <- confusion_lm_profiles[2, 2]/(confusion_lm_profiles[2, 2] + confusion_lm_profiles[2, 1])
specificity_lm_profiles <- confusion_lm_profiles[1, 1]/(confusion_lm_profiles[1, 1] + confusion_lm_profiles[1, 2])
# Number of trees plot:
tree_table <- data.frame(Ensemble = c(1:10000),
OBB = rf_lm_profiles_final$err.rate[, 1],
AvgAcc = 100 - ((rf_lm_profiles_final$err.rate[, 1])*100))
ggLinePlot <- ggplot(data = tree_table, aes(x = Ensemble, y = AvgAcc)) +
geom_line(linetype = "solid", size = 1.2, color = "navyblue") +
scale_x_continuous(name = "Trees") +
scale_y_continuous(name = "Average Percent Accuracy", limits=c(50, 100)) +
theme(axis.title.y = element_text(size = 40, colour = "black"),
axis.title.x = element_text(size = 40, colour = "black"),
axis.text.x  =  element_text(size = 40, colour = "black"), # Put color to the labels
axis.text.y  = element_text(size = 40, colour = "black"), # Put color to the labels
axis.line = element_line(colour = 'black', size = 1.5), # Color and thickness of axis
axis.ticks = element_line(colour = "black", size = 1.5), # Color and thickness of every axis sep.
legend.position = ("none"),
panel.background = element_rect(fill = "white"),
axis.ticks.length = unit(0.4, "cm"))
assign(paste("ntree_", names(dataframes_list)[j], sep = ""), ggLinePlot)
}
ntree_AA
ntree_DHA
ggLinePlot <- ggplot(data = tree_table, aes(x = Ensemble, y = AvgAcc)) +
geom_line(linetype = "solid", size = 1.2, color = "navyblue") +
title(names(dataframes_list)[j]) +
scale_x_continuous(name = "Trees") +
scale_y_continuous(name = "Average Percent Accuracy", limits=c(50, 100)) +
theme(axis.title.y = element_text(size = 40, colour = "black"),
axis.title.x = element_text(size = 40, colour = "black"),
axis.text.x  =  element_text(size = 40, colour = "black"), # Put color to the labels
axis.text.y  = element_text(size = 40, colour = "black"), # Put color to the labels
axis.line = element_line(colour = 'black', size = 1.5), # Color and thickness of axis
axis.ticks = element_line(colour = "black", size = 1.5), # Color and thickness of every axis sep.
legend.position = ("none"),
panel.background = element_rect(fill = "white"),
axis.ticks.length = unit(0.4, "cm"))
ggLinePlot <- ggplot(data = tree_table, aes(x = Ensemble, y = AvgAcc)) +
geom_line(linetype = "solid", size = 1.2, color = "navyblue") +
ggtitle(names(dataframes_list)[j]) +
scale_x_continuous(name = "Trees") +
scale_y_continuous(name = "Average Percent Accuracy", limits=c(50, 100)) +
theme(axis.title.y = element_text(size = 40, colour = "black"),
axis.title.x = element_text(size = 40, colour = "black"),
axis.text.x  =  element_text(size = 40, colour = "black"), # Put color to the labels
axis.text.y  = element_text(size = 40, colour = "black"), # Put color to the labels
axis.line = element_line(colour = 'black', size = 1.5), # Color and thickness of axis
axis.ticks = element_line(colour = "black", size = 1.5), # Color and thickness of every axis sep.
legend.position = ("none"),
panel.background = element_rect(fill = "white"),
axis.ticks.length = unit(0.4, "cm"))
ggLinePlot
ggLinePlot <- ggplot(data = tree_table, aes(x = Ensemble, y = AvgAcc)) +
geom_line(linetype = "solid", size = 1.2, color = "navyblue") +
ggtitle(names(dataframes_list)[j]) +
scale_x_continuous(name = "Trees") +
scale_y_continuous(name = "Average Percent Accuracy", limits=c(50, 100)) +
theme(plot.title = element_text(size = 40, colour = "black"),
plot.title.position = ("center"),
axis.title.y = element_text(size = 40, colour = "black"),
axis.title.x = element_text(size = 40, colour = "black"),
axis.text.x  =  element_text(size = 40, colour = "black"), # Put color to the labels
axis.text.y  = element_text(size = 40, colour = "black"), # Put color to the labels
axis.line = element_line(colour = 'black', size = 1.5), # Color and thickness of axis
axis.ticks = element_line(colour = "black", size = 1.5), # Color and thickness of every axis sep.
legend.position = ("none"),
panel.background = element_rect(fill = "white"),
axis.ticks.length = unit(0.4, "cm"))
ggLinePlot
ggLinePlot <- ggplot(data = tree_table, aes(x = Ensemble, y = AvgAcc)) +
geom_line(linetype = "solid", size = 1.2, color = "navyblue") +
ggtitle(names(dataframes_list)[j]) +
scale_x_continuous(name = "Trees") +
scale_y_continuous(name = "Average Percent Accuracy", limits=c(50, 100)) +
theme(plot.title = element_text(size = 40, colour = "black", hjust = 0.5),
axis.title.y = element_text(size = 40, colour = "black"),
axis.title.x = element_text(size = 40, colour = "black"),
axis.text.x  =  element_text(size = 40, colour = "black"), # Put color to the labels
axis.text.y  = element_text(size = 40, colour = "black"), # Put color to the labels
axis.line = element_line(colour = 'black', size = 1.5), # Color and thickness of axis
axis.ticks = element_line(colour = "black", size = 1.5), # Color and thickness of every axis sep.
legend.position = ("none"),
panel.background = element_rect(fill = "white"),
axis.ticks.length = unit(0.4, "cm"))
ggLinePlot
#--------------------------------------------- ML LIPIDOMIC DATA ---------------------------------------------------#
# This script compiles different machine learning methodologies to be run using lipid mediator profiling data. This
# script only includes the training step without the evaluation step.
# It runs SVM, random forest, LASSO analysis and basic general linear models.
#---> LIBRARY LOAD:
if (!require('randomForest')) install.packages('randomForest'); library('randomForest')
if (!require('ggplot2')) install.packages('ggplot2'); library('ggplot2')
if (!require('snowfall')) install.packages('snowfall')
if (!require('neldermead')) install.packages('neldermead')
if (!require('optimbase')) install.packages('optimbase')
if (!require('classyfire')) install.packages("classyfire_0.1-2.tar.gz",
repos = NULL,
type = "source"); library('classyfire')
set.seed(415) # To get same results even with the random part.
#---> INPUT FILE:
lm_profile <- read.table(
file = "../input/2_randomForest_(RF_models)_data.tsv",
header = TRUE,
row.names = 1, # Specify that the first column is row names.
sep = "\t",
stringsAsFactors = FALSE)
# Separates the profiles data by lipid mediators types:
substrates <- unique(unname(unlist(lm_profile[1, -1])))
# Creates data frames for each subset:
dataframes_list <- list("ALL" = lm_profile)
for (i in 1:length(substrates)) {
substrate <- lm_profile[ ,lm_profile[1, ]== substrates[[i]]]
assign(substrates[[i]], substrate)
}
if (exists('DHA') == TRUE) {dataframes_list[["DHA"]] <- DHA}
if (exists('n3DPA') == TRUE) {dataframes_list[["n3DPA"]] <- n3DPA}
if (exists('RvE') == TRUE) {dataframes_list[["RvE"]] <- RvE}
if (exists('AA') == TRUE) {dataframes_list[["AA"]] <- AA}
# Creates the data frame that is going to collect all the info regarding the models:
final_table <- data.frame(machine_learning = "del",
groups = "del",
percentage_accuracy = 1,
sensitivity = 1,
specificity = 1,
TP_per = 1,
FP_per = 1,
TN_per = 1,
FN_per = 1,
stringsAsFactors = FALSE)
for (j in 1:length(dataframes_list)) {
lm_profiles <- dataframes_list[[j]]
# Save all the values as numeric:
lm_profile_number <- sapply(lm_profiles[-1, -1], function(x) as.numeric(x))
row.names(lm_profile_number) <- row.names(lm_profiles[-1, -1])
# Scale the data because is better when you are running Machine Learning models:
lm_profiles_scale <- as.data.frame(scale(lm_profile_number, center = FALSE, scale = TRUE))
# If all the values from one column are the same, the scalation will give you NA. For those cases, to avoid errors,
# replace the NA for zeros.
lm_profiles_scale[is.na(lm_profiles_scale)] <- 0
# Add the classification variable to the data frame (Responder and non responder):
# Getting the explanatory (x) and response (y) variable. By explanatory, it means all the data that can explain why a
# patient response or not to the treatment (the lipid meadiator profiles) and the response variable is if the
# patients response or not to the treatment. In random Forest you have to create a formula where the response
# variable is explain in terms of the explanatory variable (responses ~ everything else).
lm_profiles_scale$responses <- factor(lm_profile[-1, ]$groups)
# Make sure that column names do not represent a problem to randomForest making them a valid name to R.
names(lm_profiles_scale) <- make.names(names(lm_profiles_scale))
#---> MACHINE LEARNING (randomForest R):
# In Random Forests the idea is to decorrelate the several trees which are generated on the different bootstrapped
# samples from training Data and then reduce the variance in the trees by averaging them.
# Averaging the trees also improve the perfomance of decision trees on Test Set and eventually avoid overfitting.
# The idea is to build lots of trees in such a way to make the correlation between the trees smaller.
# BEST MTRY:
# mtry is the number of variables available for splitting at each tree node. Random Forest creates several trees,
# each one using different variables to create the best version of it. With mtry we can define how many variables
# the data is split to create the different trees.
# More: https://stats.stackexchange.com/questions/102867/random-forest-mtry-question
# In this case we defined the lipid mediators to create a loop to define which mtry is the best one for our model.
oob_error <- double(ncol(lm_profiles_scale) - 1) #Define number of variable. -1 is because the last column is responses.
for (mtry in 1:(ncol(lm_profiles_scale) - 1)) {
# NOTE:
# importance = TRUE creates the plot of the important variables, that can gave us an idea, based on the
# decrease of the accuracy of the models, what lipid mediators are contributing to make a better model.
rf_lm_profiles_scales <- randomForest(responses ~ ., data = lm_profiles_scale, mtry = mtry,
importance = TRUE, ntree = 10000)
oob_error[mtry] <- 100 - ((rf_lm_profiles_scales$err.rate[10000])*100)
}
# Define the best mtry according to the best prediction value.
final_mtry <- which.max(oob_error)
# Run the model again with the right mtry value.
rf_lm_profiles_final <- randomForest(responses ~ ., data = lm_profiles_scale, mtry = final_mtry,
importance = TRUE, ntree = 10000)
# Get the confusion matrix of the model, sensitivity and specificity:
confusion_lm_profiles <- as.data.frame(rf_lm_profiles_final$confusion)
confusion_lm_profiles[is.na(confusion_lm_profiles)] <- 0
# Calculates sensitivity, specificity and AUC.
sensitivity_lm_profiles <- confusion_lm_profiles[2, 2]/(confusion_lm_profiles[2, 2] + confusion_lm_profiles[2, 1])
specificity_lm_profiles <- confusion_lm_profiles[1, 1]/(confusion_lm_profiles[1, 1] + confusion_lm_profiles[1, 2])
# Final table for random forest:
no_oob_error_table <- data.frame(machine_learning = "randomForest",
groups = names(dataframes_list)[j],
percentage_accuracy = 100 - ((rf_lm_profiles_final$err.rate[10000])*100),
sensitivity = sensitivity_lm_profiles,
specificity = specificity_lm_profiles,
TP_per = (1 - confusion_lm_profiles[2, 3])*100,
FP_per = confusion_lm_profiles[1, 3]*100,
TN_per = (1 -confusion_lm_profiles[1, 3])*100,
FN_per = confusion_lm_profiles[2, 3]*100,
stringsAsFactors = FALSE)
final_table <- rbind(final_table, no_oob_error_table)
# Number of trees plot:
tree_table <- data.frame(Ensemble = c(1:10000),
OBB = rf_lm_profiles_final$err.rate[, 1],
AvgAcc = 100 - ((rf_lm_profiles_final$err.rate[, 1])*100))
ggLinePlot <- ggplot(data = tree_table, aes(x = Ensemble, y = AvgAcc)) +
geom_line(linetype = "solid", size = 1.2, color = "navyblue") +
ggtitle(names(dataframes_list)[j]) +
scale_x_continuous(name = "Trees") +
scale_y_continuous(name = "Average Percent Accuracy", limits=c(50, 100)) +
theme(plot.title = element_text(size = 40, colour = "black", hjust = 0.5),
axis.title.y = element_text(size = 40, colour = "black"),
axis.title.x = element_text(size = 40, colour = "black"),
axis.text.x  =  element_text(size = 40, colour = "black"), # Put color to the labels
axis.text.y  = element_text(size = 40, colour = "black"), # Put color to the labels
axis.line = element_line(colour = 'black', size = 1.5), # Color and thickness of axis
axis.ticks = element_line(colour = "black", size = 1.5), # Color and thickness of every axis sep.
legend.position = ("none"),
panel.background = element_rect(fill = "white"),
axis.ticks.length = unit(0.4, "cm"))
assign(paste("ntree_", names(dataframes_list)[j], sep = ""), ggLinePlot)
}
final_table <- final_table[-1, ]
View(final_table)
table <- read.table(
file = "../../../../PhD/9 months report/Machine learning table.tsv",
header = TRUE,
sep = "\t")
View(table)
View(final_table)
table <- data.fram(model = final_table$groups,
methodoly = final_table$machine_learning,
accuracy = round(final_table$percentage_accuracy, 0))
table <- data.frame(model = final_table$groups,
methodoly = final_table$machine_learning,
accuracy = round(final_table$percentage_accuracy, 0))
View(table)
accuracy <- ggplot(data = table, aes(x = Model, y = accuracy, fill = methodology)) +
geom_bar(stat = "identity",  position = position_dodge2(preserve = 'single'), colour = "black", size = 2) +
scale_fill_manual(values=c('goldenrod1','lightslategray')) +
geom_text(position = position_dodge(w = 0.9), vjust = -0.7,
aes(label = paste(accuracy, "%", sep = "")), size = 15) +
scale_y_continuous(name = "% Accuracy Score", breaks = seq(from = 0, to = 100, by = 20),
expand = c(0, 1)) + # Expand to delete the space between zero and the x-axis.
coord_cartesian(ylim = c(1, 100)) +
theme(axis.title = element_text(size = 40),
axis.title.x = element_blank(),
axis.text.x  =  element_text(size = 40, hjust = 1, angle = 45, colour = "black"), # Put color to the labels
axis.text.y  = element_text(size = 40, hjust = 1, colour = "black"), # Put color to the labels
axis.line = element_line(colour = 'black', size = 1.5), # Color and thickness of axis
axis.ticks = element_line(colour = "black", size = 1.5), # Color and thickness of every axis sep.
panel.background = element_rect(fill = "white"),
legend.title = element_blank(),
legend.position = "top",
legend.key.size = unit(1.3, "cm"),
legend.text  = element_text(size = 35),
legend.spacing.x = unit(1, "cm"),
axis.ticks.length = unit(0.4, "cm"))
accuracy
table <- data.frame(Model = final_table$groups,
methodoly = final_table$machine_learning,
accuracy = round(final_table$percentage_accuracy, 0))
accuracy <- ggplot(data = table, aes(x = Model, y = accuracy, fill = methodology)) +
geom_bar(stat = "identity",  position = position_dodge2(preserve = 'single'), colour = "black", size = 2) +
scale_fill_manual(values=c('goldenrod1','lightslategray')) +
geom_text(position = position_dodge(w = 0.9), vjust = -0.7,
aes(label = paste(accuracy, "%", sep = "")), size = 15) +
scale_y_continuous(name = "% Accuracy Score", breaks = seq(from = 0, to = 100, by = 20),
expand = c(0, 1)) + # Expand to delete the space between zero and the x-axis.
coord_cartesian(ylim = c(1, 100)) +
theme(axis.title = element_text(size = 40),
axis.title.x = element_blank(),
axis.text.x  =  element_text(size = 40, hjust = 1, angle = 45, colour = "black"), # Put color to the labels
axis.text.y  = element_text(size = 40, hjust = 1, colour = "black"), # Put color to the labels
axis.line = element_line(colour = 'black', size = 1.5), # Color and thickness of axis
axis.ticks = element_line(colour = "black", size = 1.5), # Color and thickness of every axis sep.
panel.background = element_rect(fill = "white"),
legend.title = element_blank(),
legend.position = "top",
legend.key.size = unit(1.3, "cm"),
legend.text  = element_text(size = 35),
legend.spacing.x = unit(1, "cm"),
axis.ticks.length = unit(0.4, "cm"))
accuracy
table <- data.frame(Model = final_table$groups,
methodology = final_table$machine_learning,
accuracy = round(final_table$percentage_accuracy, 0))
accuracy <- ggplot(data = table, aes(x = Model, y = accuracy, fill = methodology)) +
geom_bar(stat = "identity",  position = position_dodge2(preserve = 'single'), colour = "black", size = 2) +
scale_fill_manual(values=c('goldenrod1','lightslategray')) +
geom_text(position = position_dodge(w = 0.9), vjust = -0.7,
aes(label = paste(accuracy, "%", sep = "")), size = 15) +
scale_y_continuous(name = "% Accuracy Score", breaks = seq(from = 0, to = 100, by = 20),
expand = c(0, 1)) + # Expand to delete the space between zero and the x-axis.
coord_cartesian(ylim = c(1, 100)) +
theme(axis.title = element_text(size = 40),
axis.title.x = element_blank(),
axis.text.x  =  element_text(size = 40, hjust = 1, angle = 45, colour = "black"), # Put color to the labels
axis.text.y  = element_text(size = 40, hjust = 1, colour = "black"), # Put color to the labels
axis.line = element_line(colour = 'black', size = 1.5), # Color and thickness of axis
axis.ticks = element_line(colour = "black", size = 1.5), # Color and thickness of every axis sep.
panel.background = element_rect(fill = "white"),
legend.title = element_blank(),
legend.position = "top",
legend.key.size = unit(1.3, "cm"),
legend.text  = element_text(size = 35),
legend.spacing.x = unit(1, "cm"),
axis.ticks.length = unit(0.4, "cm"))
accuracy
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
